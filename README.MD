# ğŸš€ Transaction API

A FastAPI-based transaction API with DuckDB backend for fraud detection data.

Based on this [Kaggle challenge dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)

## âœ¨ Features

- ğŸ¯ RESTful API for transaction management (CRUD operations)
- ğŸ¦† DuckDB database backend
- ğŸ“Š Batch processing for Parquet data ingestion
- ğŸ““ Notebook demonstrating the features of the application

## ğŸ³ Docker Setup

### ğŸ“‹ Prerequisites

- Docker
- Docker Compose

### ğŸš€ Quick Start

1. **Build and run with Docker Compose:**

   Use this specific approach to ensure that the test suite runs before the API itself:

   ```bash
   docker-compose --profile test up --build test api
   ```
   After that, you can curl the endpoints or, just use the `demonstration notebook`

## ğŸ”Œ API Endpoints

- `GET /` - API information
- `POST /transactions` - Create a new transaction
- `GET /transactions` - List all transactions
- `GET /transactions/{id}` - Get a specific transaction
- `PUT /transactions/{id}` - Update a transaction
- `DELETE /transactions/{id}` - Delete a transaction

## ğŸ“Š Data Structure

For this scenario, we'll work with the following assumptions:

The dataset should contain:
- Time series features (time, v1-v28)
- Amount

The following columns will be automatically created. For batch ingestion, the Parquet file should not have these columns:

- id
- created_at

## ğŸ› ï¸ Development

The Docker setup includes volume mounts for:
- `./src` - Source code (hot reload)
- `./database` - DuckDB database files
- `./datasets` - Data files and Parquet datasets

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/          # FastAPI application
â”‚   â”œâ”€â”€ batch/        # Batch processing
â”‚   â”œâ”€â”€ database/     # Database files
â”‚   â””â”€â”€ datasets/     # Data files
â””â”€â”€ Demonstration.ipynb  # Testing notebook
```
